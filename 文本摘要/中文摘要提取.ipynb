{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dynamic-vehicle",
   "metadata": {},
   "outputs": [],
   "source": [
    "#这次对小数据集进行词级别的机器翻译\n",
    "import torch\n",
    "import torch.utils.data as da\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import jieba\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import random\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "strange-cooperative",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "clear-tiffany",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:5\" if torch.cuda.is_available() else \"cpu\")\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--data_path', type=str, default='/data/renhongjie/文本摘要-指针网络/中文-新闻标题数据集/新闻标题数据集/train_text.txt',\n",
    "                   help='数据集特征路径')\n",
    "parser.add_argument('--label_path', type=str, default='/data/renhongjie/文本摘要-指针网络/中文-新闻标题数据集/新闻标题数据集/train_label.txt',\n",
    "                   help='数据集标签路径')\n",
    "parser.add_argument('--seq_len', type=int, default=60,\n",
    "                   help='句子最大的长度，多则截断，少则补<unk>')\n",
    "parser.add_argument('--pad', type=str, default='<unk>',\n",
    "                   help='填充词')\n",
    "parser.add_argument('--batch_size',type=int,default=32,\n",
    "                   help='每轮批次')\n",
    "parser.add_argument('--en_embedding_dim',type=int,default=256,\n",
    "                   help='编码器词向量的长度')\n",
    "parser.add_argument('--en_hidden_dim',type=int,default=512,\n",
    "                   help='编码器深度')\n",
    "parser.add_argument('--en_n_layers',type=int,default=2,\n",
    "                   help='编码器LSTM的层数')\n",
    "parser.add_argument('--de_embedding_dim',type=int,default=256,\n",
    "                   help='解码器词向量的长度')\n",
    "parser.add_argument('--de_hidden_dim',type=int,default=512,\n",
    "                   help='解码器深度')\n",
    "parser.add_argument('--de_n_layers',type=int,default=2,\n",
    "                   help='解码器LSTM的层数')\n",
    "parser.add_argument('--dropout',type=int,default=0.5,\n",
    "                   help='droput')\n",
    "parser.add_argument('--lr',type=int,default=0.0001,\n",
    "                   help='学习率')\n",
    "parser.add_argument('--weight_decay',type=int,default=0.1,\n",
    "                   help='权重衰减/L2范式')\n",
    "parser.add_argument('--clip',type=int,default=1,\n",
    "                   help='梯度剪切')\n",
    "parser.add_argument('--epochs',type=int,default=200,\n",
    "                   help='训练轮数')\n",
    "parser.add_argument('--attn_method',type=str,default='general',\n",
    "                   help='启用注意力机制')\n",
    "\n",
    "#kwargs = parser.parse_args()\n",
    "kwargs = parser.parse_known_args()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "indonesian-ensemble",
   "metadata": {},
   "outputs": [],
   "source": [
    "#对两个数据集的每句话进行分词\n",
    "def get_token_data(data,label):\n",
    "    data_token = []\n",
    "    label_token = []\n",
    "    for d,l in tqdm(zip(data,label)):\n",
    "        d,l=get_token_text(d,l)\n",
    "        data_token.append(d)\n",
    "        label_token.append(l)\n",
    "    return data_token,label_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bridal-homeless",
   "metadata": {},
   "outputs": [],
   "source": [
    "#对单句话进行分词\n",
    "def get_token_text(d,l):\n",
    "    #简单的句子处理\n",
    "    d=d.replace('.',' .').replace('?',' ?').replace('!',' !').replace(',',' , ')\n",
    "    l=l.replace('.',' .').replace('?',' ?').replace('!',' !').replace(',',' , ')\n",
    "    #中文一般用jieba\n",
    "    token_data = [st for st in jieba.cut(d)]\n",
    "    token_label = [st for st in jieba.cut(l)]\n",
    "    return [token_data,token_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "indian-tracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(data):\n",
    "    data=[j for i in data for j in i]\n",
    "    top_1000 = Counter(data).most_common(200000)\n",
    "    vocab = {x[0]:x[1] for x in top_1000}\n",
    "    word_to_idx = {word: i + 4 for i, word in enumerate(vocab)}\n",
    "    word_to_idx['<pad>'] = 0\n",
    "    word_to_idx['<sos>'] = 1\n",
    "    word_to_idx['<eos>'] = 2\n",
    "    word_to_idx['<unk>'] = 3\n",
    "    idx_to_word = {i + 4: word for i, word in enumerate(vocab)}\n",
    "    idx_to_word[0] = '<pad>'\n",
    "    idx_to_word[1] = '<sos>'\n",
    "    idx_to_word[2] = '<eos>'\n",
    "    idx_to_word[3] = '<unk>'\n",
    "    vocab_size=len(idx_to_word)\n",
    "    return vocab, vocab_size, word_to_idx, idx_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "center-option",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 转化为索引\n",
    "def encode_st(token_data, word_to_idx):\n",
    "    features = []\n",
    "    for sample in token_data:\n",
    "        feature = []\n",
    "        feature.append(1)\n",
    "        for token in sample:\n",
    "            if token in word_to_idx:\n",
    "                feature.append(word_to_idx[token])\n",
    "            else:#如果是词库中不存在的，认为是<unk>，即0\n",
    "                feature.append(0)\n",
    "        feature.append(2)\n",
    "        features.append(feature)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "musical-parish",
   "metadata": {},
   "outputs": [],
   "source": [
    "#用一个小数据集合先玩玩\n",
    "with open(kwargs.data_path, 'r', encoding='utf-8') as f:\n",
    "    data = f.read()\n",
    "\n",
    "with open(kwargs.label_path, 'r', encoding='utf-8') as f:\n",
    "    label = f.read()\n",
    "data = data.strip().split('\\n')\n",
    "label= label.strip().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "demonstrated-distribution",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "679898\n"
     ]
    }
   ],
   "source": [
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "sustainable-knowing",
   "metadata": {},
   "outputs": [],
   "source": [
    "datanew=[]\n",
    "labelnew=[]\n",
    "for i,j in zip(data,label):\n",
    "    if len(i)>120 or len(j)>30:\n",
    "        continue\n",
    "    datanew.append(i)\n",
    "    labelnew.append(j)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "nasty-income",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "229512\n"
     ]
    }
   ],
   "source": [
    "print(len(datanew))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "respective-thanksgiving",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Dump cache file failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/renhongjie/miniconda3/envs/rencaiji/lib/python3.6/site-packages/jieba/__init__.py\", line 154, in initialize\n",
      "    _replace_file(fpath, cache_file)\n",
      "PermissionError: [Errno 1] Operation not permitted: '/tmp/tmpocxhsidp' -> '/tmp/jieba.cache'\n",
      "Loading model cost 0.918 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "229512it [01:54, 2000.23it/s]\n"
     ]
    }
   ],
   "source": [
    "#此时为二维数组，每个数组是之前每句话的分词结果，词级别\n",
    "data,label=get_token_data(datanew,labelnew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "exact-daughter",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab,vocab_size, word_to_idx, idx_to_word=get_vocab(data)#682626\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "legitimate-freeware",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs.vocab_size=vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "approved-flash",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200004\n"
     ]
    }
   ],
   "source": [
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "olympic-short",
   "metadata": {},
   "outputs": [],
   "source": [
    "#此时为二维数组，每个数组是之前每句话的分词结果，词级别\n",
    "token_data=encode_st((data),word_to_idx)\n",
    "token_label=encode_st((label),word_to_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "frequent-layer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 40872, 27032, 6819, 32, 16, 163, 859, 124, 31310, 9739, 35, 14138, 34, 2]]\n",
      "[[1, 5306, 7640, 4, 12219, 27739, 4, 6819, 32, 1287, 72, 163, 17461, 31310, 9739, 1526, 15014, 11906, 28, 5020, 4, 15387, 5481, 4, 6819, 32, 22799, 21, 10352, 40872, 27032, 35, 331, 112, 30293, 34, 45335, 39056, 74, 7, 105, 104, 74, 6, 5, 5, 5, 2]]\n"
     ]
    }
   ],
   "source": [
    "print(token_label[0:1])\n",
    "print(token_data[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "careful-idaho",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#看一下预料的长度分布，然后选择一个合适的seq_len\n",
    "en_data_length=[]\n",
    "for i in token_data:\n",
    "    en_data_length.append(len(i))\n",
    "n, bins, patches = plt.hist(en_data_length, bins=180, edgecolor='None',facecolor='red') \n",
    "plt.show()\n",
    "zh_data_length=[]\n",
    "for i in token_label:\n",
    "    zh_data_length.append(len(i))\n",
    "n, bins, patches = plt.hist(zh_data_length, bins=180, edgecolor='None',facecolor='red') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "spoken-collins",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('token_data.npy', 'wb') as file1:\n",
    "    pickle.dump(token_data, file1)\n",
    "with open('token_label.npy', 'wb') as file2:\n",
    "    pickle.dump(token_label, file2)\n",
    "with open('idx_to_word.npy', 'wb') as file3:\n",
    "    pickle.dump(idx_to_word, file3)\n",
    "with open('word_to_idx.npy', 'wb') as file0:\n",
    "    pickle.dump(word_to_idx, file0)\n",
    "file0.close()\n",
    "file1.close()\n",
    "file2.close()\n",
    "file3.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "naughty-tuition",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('token_data.npy', 'rb') as file4:\n",
    "    token_data=pickle.load(file4)\n",
    "with open('token_label.npy', 'rb') as file5:\n",
    "    token_label=pickle.load(file5)\n",
    "with open('idx_to_word.npy', 'rb') as file6:\n",
    "    idx_to_word=pickle.load(file6)\n",
    "with open('word_to_idx.npy', 'rb') as file7:\n",
    "    word_to_idx=pickle.load(file7)\n",
    "kwargs.vocab_size=len(idx_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "julian-framing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "229512\n"
     ]
    }
   ],
   "source": [
    "print(len(token_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "binding-royalty",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, data, label):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "\n",
    "        assert len(data) == len(label), \\\n",
    "            \"numbers of src_data  and trg_data must be equal!\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data_sample =self.data[idx]\n",
    "        data_len = len(self.data[idx])\n",
    "        label_sample = self.label[idx]\n",
    "        label_len = len(self.label[idx])\n",
    "        return {\"data\": data_sample, \"data_len\": data_len, \"label\": label_sample, \"label_len\": label_len}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "level-result",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_batch(batch):\n",
    "    \"\"\"\n",
    "    input: -> list of dict\n",
    "        [{'src': [1, 2, 3], 'trg': [1, 2, 3]}, {'src': [1, 2, 2, 3], 'trg': [1, 2, 2, 3]}]\n",
    "    output: -> dict of tensor \n",
    "        {\n",
    "            \"src\": [[1, 2, 3, 0], [1, 2, 2, 3]].T\n",
    "            \"trg\": [[1, 2, 3, 0], [1, 2, 2, 3]].T\n",
    "        }\n",
    "    \"\"\"\n",
    "    data_lens = [d[\"data_len\"] for d in batch]\n",
    "    label_lens = [d[\"label_len\"] for d in batch]\n",
    "    \n",
    "    data_max = max([d[\"data_len\"] for d in batch])\n",
    "    label_max = max([d[\"label_len\"] for d in batch])\n",
    "    for d in batch:\n",
    "        word_to_idx\n",
    "        d[\"data\"].extend([word_to_idx[\"<pad>\"]]*(data_max-d[\"data_len\"]))\n",
    "        d[\"label\"].extend([word_to_idx[\"<pad>\"]]*(label_max-d[\"label_len\"]))\n",
    "    data = torch.tensor([pair[\"data\"] for pair in batch], dtype=torch.long, device=device)\n",
    "    label = torch.tensor([pair[\"label\"] for pair in batch], dtype=torch.long, device=device)\n",
    "    \n",
    "    batch = {\"data\":data.T, \"data_len\":data_lens, \"label\":label.T, \"label_len\":label_lens}\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "african-springfield",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据集\n",
    "train_set = TranslationDataset(token_data, token_label)\n",
    "train_iter = DataLoader(train_set, batch_size=16, collate_fn=padding_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "refined-madagascar",
   "metadata": {},
   "outputs": [],
   "source": [
    "#开始模型部分\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.input_dim = args.vocab_size\n",
    "        self.emb_dim = args.en_embedding_dim\n",
    "        self.hid_dim = args.en_hidden_dim\n",
    "        self.n_layers = args.en_n_layers\n",
    "        self.dropout = args.dropout\n",
    "        self.embedding = nn.Embedding(self.input_dim, self.emb_dim)   \n",
    "        self.rnn = nn.LSTM(input_size=self.emb_dim, hidden_size=self.hid_dim, num_layers=self.n_layers,\n",
    "                           dropout=self.dropout,bidirectional=True)\n",
    "\n",
    "        \n",
    "    def forward(self, x,input_lengths):\n",
    "        embedded = (self.embedding(x))\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths, enforce_sorted=False)\n",
    "        outputs, (hidden, cell) = self.rnn(packed)\n",
    "        outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(sequence=outputs)\n",
    "        return outputs,hidden, cell\n",
    "class Attn(nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        self.method = method\n",
    "        if self.method not in ['dot', 'general', 'concat']:\n",
    "            raise ValueError(self.method, \"is not an appropriate attention method.\")\n",
    "        self.hidden_size = hidden_size\n",
    "        if self.method == 'general':\n",
    "            self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
    "        elif self.method == 'concat':\n",
    "            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "            self.v = nn.Parameter(torch.FloatTensor(hidden_size))\n",
    "\n",
    "    def dot_score(self, hidden, encoder_output):\n",
    "        return torch.sum(hidden * encoder_output, dim=2)  # [seq_len, batch]\n",
    "\n",
    "    def general_score(self, hidden, encoder_output):\n",
    "        energy = self.attn(encoder_output)  # [seq_len, batch, hid_dim]\n",
    "        return torch.sum(hidden * energy, dim=2)  # [seq_len, batch]\n",
    "\n",
    "    def concat_score(self, hidden, encoder_output):\n",
    "        # hidden.expand(encoder_output.size(0), -1, -1) -> [seq_len, batch, N]\n",
    "        energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2)).tanh()\n",
    "        # energy = [sql_len, batch, hidden_size]\n",
    "        return torch.sum(self.v * energy, dim=2)  # [seq_len, batch]\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # hidden = [1, batch,  n_directions * hid_dim]\n",
    "        # encoder_outputs = [seq_len, batch, hid dim * n directions]\n",
    "        if self.method == 'general':\n",
    "            attn_energies = self.general_score(hidden, encoder_outputs)\n",
    "        elif self.method == 'concat':\n",
    "            attn_energies = self.concat_score(hidden, encoder_outputs)\n",
    "        elif self.method == 'dot':\n",
    "            attn_energies = self.dot_score(hidden, encoder_outputs)\n",
    "\n",
    "        attn_energies = attn_energies.t()  # [batch, seq_len]\n",
    " \n",
    "        return F.softmax(attn_energies, dim=1).unsqueeze(1)  # softmax归一化# [batch, 1, seq_len]\n",
    "class DecoderAtten(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.attn_method=args.attn_method\n",
    "        self.emb_dim = args.de_embedding_dim\n",
    "        self.hid_dim = args.de_hidden_dim\n",
    "        self.output_dim = args.vocab_size\n",
    "        self.n_layers = args.de_n_layers\n",
    "        self.dropout = args.dropout\n",
    "        self.embedding = nn.Embedding(self.output_dim, self.emb_dim)\n",
    "        self.rnn = nn.LSTM(input_size=self.emb_dim, hidden_size=self.hid_dim,num_layers= self.n_layers,\n",
    "                           dropout=self.dropout,bidirectional=True)\n",
    "        \n",
    "        bidirectional=True\n",
    "        if bidirectional:\n",
    "            self.concat = nn.Linear(self.hid_dim * 2 * 2, self.hid_dim*2)\n",
    "            self.out = nn.Linear(self.hid_dim*2, self.output_dim)\n",
    "            self.attn = Attn(self.attn_method, self.hid_dim*2)\n",
    "            self.prt=nn.Linear(self.hid_dim * 2,1)\n",
    "        else:\n",
    "            self.concat = nn.Linear(self.hid_dim * 2, self.hid_dim)\n",
    "            self.out = nn.Linear(self.hid_dim, self.output_dim)\n",
    "            self.attn = Attn(self.attn_method, self.hid_dim)\n",
    "            self.prt=nn.Linear(self.hid_dim,1)\n",
    "        self.de_dropout = nn.Dropout(args.dropout)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, x, hidden, cell,encoder_outputs):\n",
    "        batch_size = x.size(0)\n",
    "        embedded = self.embedding(x)\n",
    "        embedded = self.de_dropout(embedded)\n",
    "        embedded = embedded.view(1, batch_size, -1) # [1, B, hid_dim]\n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))    \n",
    "        \n",
    "        attn_weights = self.attn(output, encoder_outputs)\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
    "        \n",
    "        output = output.squeeze(0) # [batch, n_directions * hid_dim]\n",
    "        context = context.squeeze(1)       # [batch, n_directions * hid_dim]\n",
    "        concat_input = torch.cat((output, context), 1)  # [batch, n_directions * hid_dim * 2]\n",
    "        concat_output = torch.tanh(self.concat(concat_input))  # [batch, n_directions*hid_dim]\n",
    "\n",
    "        \n",
    "        \n",
    "        output = self.out(concat_output)\n",
    "        #prediction = self.softmax(output)\n",
    "        #return prediction, hidden, cell,attn_weights\n",
    "        #context用于计算选择网络的参数\n",
    "        prob_ptr=F.sigmoid(self.prt(context))\n",
    "        return output, hidden, cell,attn_weights,prob_ptr\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, args,device):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(args)\n",
    "       \n",
    "        self.decoder = DecoderAtten(args)\n",
    "        \n",
    "        self.device = device\n",
    "        assert self.encoder.hid_dim == self.decoder.hid_dim, \\\n",
    "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        assert self.encoder.n_layers == self.decoder.n_layers, \\\n",
    "            \"Encoder and decoder must have equal number of layers!\"\n",
    "        assert self.encoder.rnn.bidirectional == self.decoder.rnn.bidirectional, \\\n",
    "            \"Decoder and encoder must had same value of bidirectional attribute!\"   \n",
    "    def forward(self, data, label, data_length,label_length,pointer = True):\n",
    "        batch_size = label.shape[1]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        max_label_length = max(label_length)\n",
    "        max_data_length = max(data_length)\n",
    "        outputs = torch.zeros(max_label_length, batch_size, trg_vocab_size).to(self.device)\n",
    "        en_outputs,hidden, cell = self.encoder(data,data_length)   \n",
    "        #输入到Decoder网络的第一个字符是<sos>（句子开始标记）\n",
    "        de_input = label[0]\n",
    "        data=data.transpose(1,0)\n",
    "        atten=torch.zeros(max_label_length,batch_size, max_data_length).to(self.device)\n",
    "        for t in range(max_label_length):\n",
    "            #注意前面的hidden、cell和后面的是不同的\n",
    "#             if self.attention==True:\n",
    "#                 output, hidden, cell = self.decoder(de_input, hidden, cell,en_outputs)\n",
    "#             else:\n",
    "#                 output, hidden, cell = self.decoder(de_input, hidden, cell)\n",
    "#             outputs[t] = output\n",
    "            \n",
    "            if not pointer:\n",
    "                    output, decoder_hidden, cell,decoder_attn,_ = self.decoder(de_input, hidden, cell,en_outputs)\n",
    "                    decoder_attn=decoder_attn.squeeze(1)\n",
    "                    topv, topi = output.topk(1)\n",
    "                    outputs[t] = output\n",
    "                    output = topi.squeeze(1)\n",
    "            else:\n",
    "                    output, decoder_hidden, cell,decoder_attn,prob_ptr = self.decoder(de_input, hidden, cell,en_outputs)\n",
    "                    # [batch, 1]\n",
    "                    decoder_attn=decoder_attn.squeeze(1)\n",
    "                    prob_gen = 1 - prob_ptr\n",
    "                    output=output*prob_gen\n",
    "#                     print(prob_ptr.shape,decoder_attn.shape)\n",
    "#                     print(data.shape,(prob_ptr*decoder_attn).shape,output.shape)\n",
    "                    output.scatter_add_(1,data,prob_ptr*decoder_attn)\n",
    "                    topv, topi = output.topk(1)\n",
    "                    outputs[t] = output\n",
    "                    output = topi.squeeze(1)  #\n",
    "#             print('decoder_attn',decoder_attn.shape)\n",
    "#             print('atten',atten.shape)\n",
    "#             print('output',output.shape)\n",
    "#             print('decoder_hidden',decoder_hidden.shape)\n",
    "#             print('outputs',outputs.shape)\n",
    "#             print('en_outputs',en_outputs.shape)\n",
    "#             print('hidden',hidden.shape)\n",
    "            \n",
    "#             print('output',output)\n",
    "#             print('outputs',outputs[t])\n",
    "            atten[t]=(decoder_attn)\n",
    "#             os.exit()\n",
    "        return outputs,atten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "naval-balance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #开始模型部分\n",
    "# class Encoder(nn.Module):\n",
    "#     def __init__(self, args):\n",
    "#         super().__init__()\n",
    "#         self.input_dim = args.vocab_size\n",
    "#         self.emb_dim = args.en_embedding_dim\n",
    "#         self.hid_dim = args.en_hidden_dim\n",
    "#         self.n_layers = args.en_n_layers\n",
    "#         self.dropout = args.dropout\n",
    "#         self.embedding = nn.Embedding(self.input_dim, self.emb_dim)   \n",
    "#         self.rnn = nn.LSTM(input_size=self.emb_dim, hidden_size=self.hid_dim, num_layers=self.n_layers,\n",
    "#                            dropout=self.dropout,bidirectional=True)\n",
    "\n",
    "        \n",
    "#     def forward(self, x,input_lengths):\n",
    "#         embedded = (self.embedding(x))\n",
    "#         packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths, enforce_sorted=False)\n",
    "#         outputs, (hidden, cell) = self.rnn(packed)\n",
    "#         outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(sequence=outputs)\n",
    "#         return outputs,hidden, cell\n",
    "# class Attn(nn.Module):\n",
    "#     def __init__(self, method, hidden_size):\n",
    "#         super(Attn, self).__init__()\n",
    "#         self.method = method\n",
    "#         if self.method not in ['dot', 'general', 'concat']:\n",
    "#             raise ValueError(self.method, \"is not an appropriate attention method.\")\n",
    "#         self.hidden_size = hidden_size\n",
    "#         if self.method == 'general':\n",
    "#             self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
    "#         elif self.method == 'concat':\n",
    "#             self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "#             self.v = nn.Parameter(torch.FloatTensor(hidden_size))\n",
    "\n",
    "#     def dot_score(self, hidden, encoder_output):\n",
    "#         return torch.sum(hidden * encoder_output, dim=2)  # [seq_len, batch]\n",
    "\n",
    "#     def general_score(self, hidden, encoder_output):\n",
    "#         energy = self.attn(encoder_output)  # [seq_len, batch, hid_dim]\n",
    "#         return torch.sum(hidden * energy, dim=2)  # [seq_len, batch]\n",
    "\n",
    "#     def concat_score(self, hidden, encoder_output):\n",
    "#         # hidden.expand(encoder_output.size(0), -1, -1) -> [seq_len, batch, N]\n",
    "#         energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2)).tanh()\n",
    "#         # energy = [sql_len, batch, hidden_size]\n",
    "#         return torch.sum(self.v * energy, dim=2)  # [seq_len, batch]\n",
    "\n",
    "#     def forward(self, hidden, encoder_outputs):\n",
    "#         # hidden = [1, batch,  n_directions * hid_dim]\n",
    "#         # encoder_outputs = [seq_len, batch, hid dim * n directions]\n",
    "#         if self.method == 'general':\n",
    "#             attn_energies = self.general_score(hidden, encoder_outputs)\n",
    "#         elif self.method == 'concat':\n",
    "#             attn_energies = self.concat_score(hidden, encoder_outputs)\n",
    "#         elif self.method == 'dot':\n",
    "#             attn_energies = self.dot_score(hidden, encoder_outputs)\n",
    "\n",
    "#         attn_energies = attn_energies.t()  # [batch, seq_len]\n",
    " \n",
    "#         return F.softmax(attn_energies, dim=1).unsqueeze(1)  # softmax归一化# [batch, 1, seq_len]\n",
    "# class DecoderAtten(nn.Module):\n",
    "#     def __init__(self, args):\n",
    "#         super().__init__()\n",
    "#         self.attn_method=args.attn_method\n",
    "#         self.emb_dim = args.de_embedding_dim\n",
    "#         self.hid_dim = args.de_hidden_dim\n",
    "#         self.output_dim = args.vocab_size\n",
    "#         self.n_layers = args.de_n_layers\n",
    "#         self.dropout = args.dropout\n",
    "#         self.embedding = nn.Embedding(self.output_dim, self.emb_dim)\n",
    "#         self.rnn = nn.LSTM(input_size=self.emb_dim, hidden_size=self.hid_dim,num_layers= self.n_layers,\n",
    "#                            dropout=self.dropout,bidirectional=True)\n",
    "        \n",
    "#         bidirectional=True\n",
    "#         if bidirectional:\n",
    "#             self.concat = nn.Linear(self.hid_dim * 2 * 2, self.hid_dim*2)\n",
    "#             self.out = nn.Linear(self.hid_dim*2, self.output_dim)\n",
    "#             self.attn = Attn(self.attn_method, self.hid_dim*2)\n",
    "#             self.prt=nn.Linear(self.hid_dim * 2,1)\n",
    "#         else:\n",
    "#             self.concat = nn.Linear(self.hid_dim * 2, self.hid_dim)\n",
    "#             self.out = nn.Linear(self.hid_dim, self.output_dim)\n",
    "#             self.attn = Attn(self.attn_method, self.hid_dim)\n",
    "#             self.prt=nn.Linear(self.hid_dim,1)\n",
    "#         self.de_dropout = nn.Dropout(args.dropout)\n",
    "#         self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "#     def forward(self, x, hidden, cell,encoder_outputs):\n",
    "#         batch_size = x.size(0)\n",
    "#         embedded = self.embedding(x)\n",
    "#         embedded = self.de_dropout(embedded)\n",
    "#         embedded = embedded.view(1, batch_size, -1) # [1, B, hid_dim]\n",
    "#         output, (hidden, cell) = self.rnn(embedded, (hidden, cell))    \n",
    "#         print('解码器里面RNN的output, (hidden, cell)输出结果：',output.shape, hidden.shape, cell.shape)\n",
    "#         attn_weights = self.attn(output, encoder_outputs)\n",
    "#         print(\"attn_weights:\",attn_weights.shape)\n",
    "#         print(\"encoder_outputs:\",encoder_outputs.shape)\n",
    "#         context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
    "#         print(\"context:\",context.shape)\n",
    "        \n",
    "#         output = output.squeeze(0) # [batch, n_directions * hid_dim]\n",
    "#         context = context.squeeze(1)       # [batch, n_directions * hid_dim]\n",
    "        \n",
    "#         concat_input = torch.cat((output, context), 1)  # [batch, n_directions * hid_dim * 2]\n",
    "#         concat_output = torch.tanh(self.concat(concat_input))  # [batch, n_directions*hid_dim]\n",
    "#         print(\"concat_input\",concat_input.shape)\n",
    "#         print(\"concat_output\",concat_output.shape)\n",
    "        \n",
    "#         output = self.out(concat_output)\n",
    "#         print(\"output\",concat_input.shape)\n",
    "#         #prediction = self.softmax(output)\n",
    "#         #return prediction, hidden, cell,attn_weights\n",
    "#         #context用于计算选择网络的参数\n",
    "#         prob_ptr=F.sigmoid(self.prt(context))\n",
    "#         print(\"prob_ptr\",prob_ptr.shape)\n",
    "#         return output, hidden, cell,attn_weights,prob_ptr\n",
    "\n",
    "# class Seq2Seq(nn.Module):\n",
    "#     def __init__(self, args,device):\n",
    "#         super().__init__()\n",
    "#         self.encoder = Encoder(args)\n",
    "       \n",
    "#         self.decoder = DecoderAtten(args)\n",
    "        \n",
    "#         self.device = device\n",
    "#         assert self.encoder.hid_dim == self.decoder.hid_dim, \\\n",
    "#             \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "#         assert self.encoder.n_layers == self.decoder.n_layers, \\\n",
    "#             \"Encoder and decoder must have equal number of layers!\"\n",
    "#         assert self.encoder.rnn.bidirectional == self.decoder.rnn.bidirectional, \\\n",
    "#             \"Decoder and encoder must had same value of bidirectional attribute!\"   \n",
    "#     def forward(self, data, label, data_length,label_length,pointer = True):\n",
    "#         print('刚进入模型的原始数据x：',data.shape)\n",
    "#         batch_size = label.shape[1]\n",
    "#         trg_vocab_size = self.decoder.output_dim\n",
    "#         max_label_length = max(label_length)\n",
    "#         max_data_length = max(data_length)\n",
    "#         outputs = torch.zeros(max_label_length, batch_size, trg_vocab_size).to(self.device)\n",
    "#         en_outputs,hidden, cell = self.encoder(data,data_length)\n",
    "#         print('经过编码器后输出的en_outputs,hidden, cell：',en_outputs.shape,hidden.shape, cell.shape)\n",
    "#         #输入到Decoder网络的第一个字符是<sos>（句子开始标记）\n",
    "#         de_input = label[0]\n",
    "#         data=data.transpose(1,0)\n",
    "#         print(\"输入模型的第一个字符：\",data.shape)\n",
    "#         atten=torch.zeros(max_label_length,batch_size, max_data_length).to(self.device)\n",
    "#         for t in range(max_label_length):\n",
    "#             #注意前面的hidden、cell和后面的是不同的\n",
    "# #             if self.attention==True:\n",
    "# #                 output, hidden, cell = self.decoder(de_input, hidden, cell,en_outputs)\n",
    "# #             else:\n",
    "# #                 output, hidden, cell = self.decoder(de_input, hidden, cell)\n",
    "# #             outputs[t] = output\n",
    "            \n",
    "#             if not pointer:\n",
    "#                     output, decoder_hidden, cell,decoder_attn,_ = self.decoder(de_input, hidden, cell,en_outputs)\n",
    "#                     decoder_attn=decoder_attn.squeeze(1)\n",
    "#                     topv, topi = output.topk(1)\n",
    "#                     outputs[t] = output\n",
    "#                     output = topi.squeeze(1)\n",
    "#             else:\n",
    "#                     output, decoder_hidden, cell,decoder_attn,prob_ptr = self.decoder(de_input, hidden, cell,en_outputs)\n",
    "#                     print(\"output, decoder_hidden, cell,decoder_attn,prob_ptr:\",output.shape, decoder_hidden.shape, cell.shape,decoder_attn.shape,prob_ptr.shape)\n",
    "#                     # [batch, 1]\n",
    "#                     decoder_attn=decoder_attn.squeeze(1)\n",
    "#                     print(\"decoder_attn\",decoder_attn.shape)\n",
    "#                     prob_gen = 1 - prob_ptr\n",
    "#                     output=output*prob_gen\n",
    "# #                     print(prob_ptr.shape,decoder_attn.shape)\n",
    "# #                     print(data.shape,(prob_ptr*decoder_attn).shape,output.shape)\n",
    "#                     output.scatter_add_(1,data,prob_ptr*decoder_attn)\n",
    "#                     print(\"指针网络后的output：\",output.shape)\n",
    "#                     topv, topi = output.topk(1)\n",
    "#                     print(\"topv, topi：\",topv.shape, topi.shape)\n",
    "#                     outputs[t] = output\n",
    "#                     output = topi.squeeze(1)  #\n",
    "            \n",
    "#             break\n",
    "# #             print('decoder_attn',decoder_attn.shape)\n",
    "# #             print('atten',atten.shape)\n",
    "# #             print('output',output.shape)\n",
    "# #             print('decoder_hidden',decoder_hidden.shape)\n",
    "# #             print('outputs',outputs.shape)\n",
    "# #             print('en_outputs',en_outputs.shape)\n",
    "# #             print('hidden',hidden.shape)\n",
    "            \n",
    "# #             print('output',output)\n",
    "# #             print('outputs',outputs[t])\n",
    "#             atten[t]=(decoder_attn)\n",
    "# #             os.exit()\n",
    "#             print('最后结果：',outputs.shape,atten.shape)\n",
    "#         return outputs,atten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "floral-priest",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args,device,model,train_iter):\n",
    "    moedel=model.to(device)\n",
    "    loss_function = nn.CrossEntropyLoss(ignore_index = 0)\n",
    "    #optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "    print(\"模型开始训练\")\n",
    "    for epoch in range(args.epochs):\n",
    "        start = time.time()\n",
    "        train_loss, train_acc = 0, 0\n",
    "        test_loss, test_acc = 0, 0\n",
    "        n,m= 0,0\n",
    "        model.train()\n",
    "        for i,(batch) in enumerate(tqdm(train_iter)):\n",
    "            n += 1\n",
    "            input_batchs = batch[\"data\"].to(device)\n",
    "            target_batchs = batch[\"label\"].to(device)\n",
    "            input_lens = batch[\"data_len\"]\n",
    "            target_lens = batch[\"label_len\"]\n",
    "            optimizer.zero_grad()\n",
    "            output,_ = model(input_batchs, target_batchs,input_lens,target_lens)\n",
    "            loss = loss_function(output.reshape(-1,output.shape[-1] ), target_batchs.reshape(-1))\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
    "            optimizer.step()\n",
    "            train_loss += loss\n",
    "        #if epoch%5==0:\n",
    "        if epoch==100000:\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                for i,(batch) in enumerate(tqdm(train_iter)):\n",
    "                    m += 1\n",
    "                    input_batchs = batch[\"data\"].to(device)\n",
    "                    target_batchs = batch[\"label\"].to(device)\n",
    "                    input_lens = batch[\"data_len\"]\n",
    "                    target_lens = batch[\"label_len\"]\n",
    "                    output,_ = model(input_batchs, target_batchs,input_lens,target_lens)\n",
    "                    loss = loss_function(output.reshape(-1,output.shape[-1] ), target_batchs.reshape(-1))\n",
    "\n",
    "                    test_loss += loss \n",
    "                end = time.time()\n",
    "                runtime = end - start\n",
    "                print(\n",
    "                    'epoch: %d, train loss: %.4f,test loss: %.4f,time: %.4f \\n' % (epoch, train_loss.data / n,test_loss.data / m,runtime)\n",
    "                )\n",
    "        else:\n",
    "            end = time.time()\n",
    "            runtime = end - start\n",
    "            print(\n",
    "                'epoch: %d, train loss: %.4f,time: %.4f \\n' % (epoch, train_loss.data / n,runtime)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "moved-penguin",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Namespace' object has no attribute 'vocab_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-48a5e8aba697>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSeq2Seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-ec7bb69917c7>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, device)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecoderAtten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-ec7bb69917c7>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memb_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0men_embedding_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhid_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0men_hidden_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Namespace' object has no attribute 'vocab_size'"
     ]
    }
   ],
   "source": [
    "model=Seq2Seq(kwargs,device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "expensive-destruction",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/14345 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型开始训练\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/renhongjie/miniconda3/envs/rencaiji/lib/python3.6/site-packages/torch/nn/functional.py:1639: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "  0%|          | 0/14345 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Assertion `THCTensor_(checkGPU)(state, 4, input, target, output, total_weight)' failed. Some of weight/gradient/input tensors are located on different GPUs. Please move them to a single one. at /pytorch/aten/src/THCUNN/generic/ClassNLLCriterion.cu:28",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-922ec4d735d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-30-64262c9bc3aa>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args, device, model, train_iter)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_batchs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_batchs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_lens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_lens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_batchs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rencaiji/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rencaiji/lib/python3.6/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    960\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m--> 962\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rencaiji/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2466\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2467\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2468\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rencaiji/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2262\u001b[0m                          .format(input.size(0), target.size(0)))\n\u001b[1;32m   2263\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2264\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2265\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2266\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Assertion `THCTensor_(checkGPU)(state, 4, input, target, output, total_weight)' failed. Some of weight/gradient/input tensors are located on different GPUs. Please move them to a single one. at /pytorch/aten/src/THCUNN/generic/ClassNLLCriterion.cu:28"
     ]
    }
   ],
   "source": [
    "train(kwargs,device,model,train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "traditional-identity",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'zhaiyao.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "concrete-auckland",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=torch.load('zhaiyao_best.pkl')\n",
    "model=model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "plastic-requirement",
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('ceshi','a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "beginning-saskatchewan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.write((\n",
    "                'epoch: %d, train loss: %.4f,time: %.4f \\n' % (1, 2 / 3,4)\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spanish-fellow",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
